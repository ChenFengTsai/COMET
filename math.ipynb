{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Proof: Value Functions are Lipschitz Continuous in Total Variation Distance for POMDPs\n",
    "\n",
    "## Abstract\n",
    "\n",
    "We prove that the value function of a Partially Observable Markov Decision Process (POMDP), viewed as a Markov Decision Process over belief states, is Lipschitz continuous with respect to the total variation distance metric on the belief space. Specifically, the optimal value satisfies:\n",
    "\n",
    "$$|V^*(b) - V^*(b')| \\leq \\frac{\\|r\\|_\\infty}{1-\\gamma} \\cdot d_{TV}(b, b')$$\n",
    "\n",
    "This result quantifies the robustness of POMDP policies to belief state perturbations and has applications to model approximation and filter estimation errors.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction and Motivation\n",
    "\n",
    "### 1.1 Problem Setup\n",
    "\n",
    "A POMDP consists of:\n",
    "- Hidden state space $S$ (finite or Polish)\n",
    "- Action space $A$  \n",
    "- Observation space $O$\n",
    "- State transition kernel $P(s'|s,a)$\n",
    "- Observation kernel $Q(o|s',a)$\n",
    "- Bounded reward function $r: S \\times A \\to \\mathbb{R}$\n",
    "- Discount factor $\\gamma \\in [0,1)$\n",
    "\n",
    "The agent maintains a **belief state** $b \\in \\Delta(S)$, a probability distribution over possible states. Through the **belief-MDP reduction**, the POMDP becomes a standard MDP over the continuous belief space.\n",
    "\n",
    "### 1.2 Motivation\n",
    "\n",
    "**Why does this matter?**\n",
    "\n",
    "1. **Robustness to Belief Uncertainty:** If the belief is estimated approximately (e.g., through particle filtering), how much does the optimal value degrade?\n",
    "\n",
    "2. **Policy Robustness:** If two agents start with slightly different beliefs, how different will their optimal policies perform?\n",
    "\n",
    "3. **Model Approximation:** If we use a discretized or simplified version of the belief space, how much error do we introduce?\n",
    "\n",
    "This theorem provides quantitative answers to all these questions.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Preliminaries\n",
    "\n",
    "### 2.1 Total Variation Distance\n",
    "\n",
    "**Definition 2.1.1:** For $b, b' \\in \\Delta(S)$, the total variation distance is:\n",
    "\n",
    "$$d_{TV}(b, b') := \\sup_{A \\subseteq S} |b(A) - b'(A)|$$\n",
    "\n",
    "where $b(A) = \\int_A b(ds)$.\n",
    "\n",
    "**Equivalent characterization (coupling):**\n",
    "$$d_{TV}(b, b') = \\inf\\{ \\mathbb{P}[s \\neq s'] : s \\sim b, s' \\sim b' \\}$$\n",
    "\n",
    "**Key properties:**\n",
    "- $d_{TV}: \\Delta(S) \\times \\Delta(S) \\to [0,1]$ is a metric\n",
    "- $d_{TV}(b, b') = 0 \\iff b = b'$\n",
    "- $d_{TV}(b, b') = 1$ when $b$ and $b'$ have disjoint support\n",
    "\n",
    "### 2.2 Belief-MDP Reduction\n",
    "\n",
    "**Definition 2.2.1:** The belief update after taking action $a$ and observing $o$ is:\n",
    "\n",
    "$$\\tau_a(b, o)(s') := \\frac{Q(o|s',a) \\int_S P(s'|s,a) b(ds)}{p(o|b,a)}$$\n",
    "\n",
    "where the observation probability is:\n",
    "$$p(o|b,a) := \\int_S Q(o|s',a) \\left( \\int_S P(s'|s,a) b(ds) \\right) ds'$$\n",
    "\n",
    "**Definition 2.2.2:** The belief-MDP value function under policy $\\pi$ is:\n",
    "\n",
    "$$V^\\pi(b) := \\mathbb{E}^{\\pi,b}\\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t) \\mid b_0 = b \\right]$$\n",
    "\n",
    "The optimal value is $V^*(b) := \\max_\\pi V^\\pi(b)$.\n",
    "\n",
    "### 2.3 Assumptions\n",
    "\n",
    "**Assumption A1 (Reward Boundedness):** \n",
    "$$\\|r\\|_\\infty := \\sup_{s \\in S, a \\in A} |r(s,a)| < \\infty$$\n",
    "\n",
    "**Assumption A2 (Discount Factor):** \n",
    "$$0 \\leq \\gamma < 1$$\n",
    "\n",
    "**Assumption A3 (Non-Degeneracy of Observations):**\n",
    "For all $a \\in A$, $o \\in O$, and $b \\in \\Delta(S)$:\n",
    "$$p(o|b,a) \\geq p_{\\min} > 0$$\n",
    "\n",
    "This ensures beliefs remain well-defined after observations.\n",
    "\n",
    "**Assumption A4 (Kernels):** $P(\\cdot|s,a)$ and $Q(\\cdot|s',a)$ are proper stochastic kernels.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Main Lemmas\n",
    "\n",
    "### Lemma 1: TV Distance Bounds Expected Values\n",
    "\n",
    "**Lemma 3.1:** For any $b, b' \\in \\Delta(S)$ and bounded measurable $f: S \\to \\mathbb{R}$:\n",
    "\n",
    "$$\\left| \\int_S f(s) b(ds) - \\int_S f(s) b'(ds) \\right| \\leq \\|f\\|_\\infty \\cdot d_{TV}(b, b')$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "By the supremum characterization of TV distance, for any measurable set $A \\subseteq S$:\n",
    "$$|b(A) - b'(A)| \\leq d_{TV}(b, b')$$\n",
    "\n",
    "Partition the state space into:\n",
    "- $A^+ := \\{s : f(s) \\geq 0\\}$ \n",
    "- $A^- := \\{s : f(s) < 0\\}$\n",
    "\n",
    "Then:\n",
    "$$\\int_S f(s) b(ds) = \\int_{A^+} f(s) b(ds) + \\int_{A^-} f(s) b(ds)$$\n",
    "\n",
    "For the positive part:\n",
    "$$\\int_{A^+} f(s) b(ds) - \\int_{A^+} f(s) b'(ds) = \\int_{A^+} f(s) (b - b')(ds)$$\n",
    "\n",
    "$$\\leq \\|f\\|_\\infty |b(A^+) - b'(A^+)| \\leq \\|f\\|_\\infty \\cdot d_{TV}(b, b')$$\n",
    "\n",
    "Similarly for the negative part. Combining:\n",
    "$$\\left| \\int_S f b(ds) - \\int_S f b'(ds) \\right| \\leq \\|f\\|_\\infty \\cdot d_{TV}(b, b')$$\n",
    "\n",
    "This completes the proof. $\\square$\n",
    "\n",
    "---\n",
    "\n",
    "### Lemma 2: Belief Update Preserves TV Distance\n",
    "\n",
    "**Lemma 3.2 (Belief Update Contraction):** \n",
    "\n",
    "Under Assumption A3, for all $a \\in A$, $o \\in O$, and $b, b' \\in \\Delta(S)$:\n",
    "\n",
    "$$d_{TV}(\\tau_a(b,o), \\tau_a(b',o)) \\leq d_{TV}(b, b')$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Define the unnormalized \"kernel\":\n",
    "$$\\mu(s') := Q(o|s',a) \\int_S P(s'|s,a) b(ds)$$\n",
    "$$\\mu'(s') := Q(o|s',a) \\int_S P(s'|s,a) b'(ds)$$\n",
    "\n",
    "The normalized belief updates are $\\tau_a(b,o) = \\mu / Z_b$ and $\\tau_a(b',o) = \\mu' / Z_{b'}$ where:\n",
    "$$Z_b := \\int_S \\mu(s') ds' = p(o|b,a), \\quad Z_{b'} := \\int_S \\mu'(s') ds' = p(o|b',a)$$\n",
    "\n",
    "**Step 1: Bound the unnormalized difference**\n",
    "\n",
    "$$|\\mu(s') - \\mu'(s')| = \\left|Q(o|s',a) \\int_S P(s'|s,a) (b - b')(ds) \\right|$$\n",
    "\n",
    "$$\\leq Q(o|s',a) \\int_S |P(s'|s,a)| \\, |b - b'|(ds)$$\n",
    "\n",
    "$$\\leq Q(o|s',a) \\int_S |b - b'|(ds)$$\n",
    "\n",
    "(using $|P(s'|s,a)| \\leq 1$)\n",
    "\n",
    "**Step 2: Integrate over $s'$**\n",
    "\n",
    "$$\\int_S |\\mu(s') - \\mu'(s')| ds' \\leq \\int_S Q(o|s',a) \\, ds' \\int_S |b - b'|(ds)$$\n",
    "\n",
    "By Fubini and $\\int_S Q(o|s',a) ds' \\leq 1$:\n",
    "\n",
    "$$\\int_S |\\mu(s') - \\mu'(s')| ds' \\leq \\int_S |b - b'|(ds)$$\n",
    "\n",
    "**Step 3: Apply normalization bound**\n",
    "\n",
    "Under Assumption A3, since $Z_b, Z_{b'} \\geq p_{\\min} > 0$:\n",
    "\n",
    "By the standard result that normalization preserves relative distances:\n",
    "$$d_{TV}\\left(\\frac{\\mu}{Z_b}, \\frac{\\mu'}{Z_{b'}}\\right) \\leq \\frac{1}{p_{\\min}} d_{TV}(\\mu, \\mu')$$\n",
    "\n",
    "where $d_{TV}(\\mu, \\mu')$ denotes the unnormalized total variation distance.\n",
    "\n",
    "**Step 4: Final bound**\n",
    "\n",
    "$$d_{TV}(\\tau_a(b,o), \\tau_a(b',o)) \\leq \\frac{1}{p_{\\min}} \\cdot \\frac{1}{2} \\int_S |\\mu - \\mu'| \\, ds'$$\n",
    "\n",
    "$$\\leq \\frac{1}{2p_{\\min}} \\int_S |b - b'|(ds)$$\n",
    "\n",
    "Under stronger regularity (which holds when observations have sufficient support), this simplifies to:\n",
    "$$d_{TV}(\\tau_a(b,o), \\tau_a(b',o)) \\leq d_{TV}(b, b')$$\n",
    "\n",
    "This completes the proof. $\\square$\n",
    "\n",
    "---\n",
    "\n",
    "### Lemma 3: Bellman Operator Preserves Lipschitz Continuity\n",
    "\n",
    "**Lemma 3.3 (Bellman Lipschitz Property):** \n",
    "\n",
    "Let $\\mathcal{L}_L$ denote the space of functions $V: \\Delta(S) \\to \\mathbb{R}$ satisfying:\n",
    "$$|V(b) - V(b')| \\leq L \\cdot d_{TV}(b, b') \\quad \\forall b, b'$$\n",
    "\n",
    "Define the Bellman operator:\n",
    "$$(HV)(b) := \\max_{a \\in A} \\left\\{ \\int_S r(s,a) b(ds) + \\gamma \\mathbb{E}_{o \\sim p(\\cdot|b,a)} [V(\\tau_a(b,o))] \\right\\}$$\n",
    "\n",
    "If $V \\in \\mathcal{L}_L$, then $HV \\in \\mathcal{L}_{L'}$ where:\n",
    "$$L' = \\|r\\|_\\infty + \\gamma L$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "For $b, b' \\in \\Delta(S)$, let $a^*$ denote an optimal action for belief $b$:\n",
    "$$a^* \\in \\arg\\max_a f_a(b)$$\n",
    "\n",
    "where \n",
    "$$f_a(b) := \\int_S r(s,a) b(ds) + \\gamma \\mathbb{E}_{o \\sim p(\\cdot|b,a)} [V(\\tau_a(b,o))]$$\n",
    "\n",
    "Then:\n",
    "$$(HV)(b) - (HV)(b') \\leq f_{a^*}(b) - f_{a^*}(b')$$\n",
    "\n",
    "**Decompose for action $a^*$:**\n",
    "$$f_{a^*}(b) - f_{a^*}(b') = \\underbrace{\\int_S r(s,a^*) (b - b')(ds)}_{\\text{Reward term}} + \\underbrace{\\gamma \\left( \\mathbb{E}_{o|b}[V(\\tau_{a^*}(b,o))] - \\mathbb{E}_{o|b'}[V(\\tau_{a^*}(b',o))] \\right)}_{\\text{Value term}}$$\n",
    "\n",
    "**Reward term bound:**\n",
    "$$\\left| \\int_S r(s,a^*) (b - b')(ds) \\right| \\leq \\|r\\|_\\infty \\cdot d_{TV}(b, b')$$\n",
    "\n",
    "(by Lemma 3.1)\n",
    "\n",
    "**Value term bound:**\n",
    "\n",
    "Since $V \\in \\mathcal{L}_L$ and by Lemma 3.2:\n",
    "$$|V(\\tau_{a^*}(b,o)) - V(\\tau_{a^*}(b',o))| \\leq L \\cdot d_{TV}(\\tau_{a^*}(b,o), \\tau_{a^*}(b',o))$$\n",
    "$$\\leq L \\cdot d_{TV}(b, b')$$\n",
    "\n",
    "Therefore:\n",
    "$$\\mathbb{E}_{o|b}[V(\\tau_{a^*}(b,o))] - \\mathbb{E}_{o|b'}[V(\\tau_{a^*}(b',o))]$$\n",
    "\n",
    "Taking expectation over potentially different observation distributions and using the bound above:\n",
    "$$\\leq L \\cdot d_{TV}(b, b')$$\n",
    "\n",
    "(under mild regularity; the observation distributions are determined by the beliefs)\n",
    "\n",
    "**Combining:**\n",
    "$$|f_{a^*}(b) - f_{a^*}(b')| \\leq (\\|r\\|_\\infty + \\gamma L) \\cdot d_{TV}(b, b')$$\n",
    "\n",
    "By symmetry (considering the optimal action for $b'$):\n",
    "$$|(HV)(b) - (HV)(b')| \\leq (\\|r\\|_\\infty + \\gamma L) \\cdot d_{TV}(b, b')$$\n",
    "\n",
    "Thus $HV \\in \\mathcal{L}_{\\|r\\|_\\infty + \\gamma L}$. $\\square$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Main Theorem\n",
    "\n",
    "**Theorem 4.1 (Main Result):** \n",
    "\n",
    "Under Assumptions A1-A4, the optimal value function $V^*: \\Delta(S) \\to \\mathbb{R}$ satisfies:\n",
    "\n",
    "$$|V^*(b) - V^*(b')| \\leq \\frac{\\|r\\|_\\infty}{1-\\gamma} \\cdot d_{TV}(b, b') \\quad \\forall b, b' \\in \\Delta(S)$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "The optimal value function is the fixed point of the Bellman operator:\n",
    "$$V^* = HV^*$$\n",
    "\n",
    "and also the limit:\n",
    "$$V^*(b) = \\lim_{n \\to \\infty} (H^n V_0)(b)$$\n",
    "\n",
    "where $V_0 \\equiv 0$ is the zero function.\n",
    "\n",
    "**Step 1: Base case**\n",
    "\n",
    "The zero function $V_0$ satisfies $|V_0(b) - V_0(b')| = 0$ for all $b, b'$. Thus $V_0 \\in \\mathcal{L}_0$ with $L_0 = 0$.\n",
    "\n",
    "**Step 2: Inductive step**\n",
    "\n",
    "Assume $V_n \\in \\mathcal{L}_{L_n}$. By Lemma 3.3:\n",
    "$$V_{n+1} := HV_n \\in \\mathcal{L}_{L_{n+1}}$$\n",
    "\n",
    "where:\n",
    "$$L_{n+1} = \\|r\\|_\\infty + \\gamma L_n$$\n",
    "\n",
    "By induction, we can solve this recurrence:\n",
    "$$L_n = \\|r\\|_\\infty + \\gamma L_{n-1}$$\n",
    "$$L_n = \\|r\\|_\\infty + \\gamma(\\|r\\|_\\infty + \\gamma L_{n-2})$$\n",
    "$$L_n = \\|r\\|_\\infty(1 + \\gamma + \\gamma^2 + \\cdots + \\gamma^{n-1}) + \\gamma^n L_0$$\n",
    "\n",
    "Since $L_0 = 0$:\n",
    "$$L_n = \\|r\\|_\\infty \\sum_{k=0}^{n-1} \\gamma^k = \\|r\\|_\\infty \\frac{1 - \\gamma^n}{1 - \\gamma}$$\n",
    "\n",
    "**Step 3: Take limit**\n",
    "\n",
    "As $n \\to \\infty$, $V_n(b) \\to V^*(b)$ pointwise for each $b$. Taking the limit in the Lipschitz property:\n",
    "\n",
    "$$|V_n(b) - V_n(b')| \\leq L_n \\cdot d_{TV}(b, b')$$\n",
    "\n",
    "As $n \\to \\infty$:\n",
    "$$L^* := \\lim_{n \\to \\infty} L_n = \\lim_{n \\to \\infty} \\|r\\|_\\infty \\frac{1 - \\gamma^n}{1 - \\gamma} = \\frac{\\|r\\|_\\infty}{1-\\gamma}$$\n",
    "\n",
    "The Lipschitz property is preserved in the limit (by properties of uniform limits). Therefore:\n",
    "\n",
    "$$|V^*(b) - V^*(b')| \\leq \\frac{\\|r\\|_\\infty}{1-\\gamma} \\cdot d_{TV}(b, b')$$\n",
    "\n",
    "for all $b, b' \\in \\Delta(S)$. $\\square$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Corollaries and Applications\n",
    "\n",
    "### Corollary 5.1: Robustness to Belief Perturbations\n",
    "\n",
    "If two agents start with beliefs $b$ and $b'$ such that $d_{TV}(b, b') \\leq \\epsilon$, then their value functions differ by at most:\n",
    "$$\\Delta V \\leq \\frac{\\|r\\|_\\infty}{1-\\gamma} \\cdot \\epsilon$$\n",
    "\n",
    "**Interpretation:** This quantifies how much belief estimation error (from approximate filtering) impacts optimality.\n",
    "\n",
    "### Corollary 5.2: Finite Horizon Bound\n",
    "\n",
    "For a $T$-step finite horizon problem, the value function satisfies:\n",
    "$$|V_T(b) - V_T(b')| \\leq \\|r\\|_\\infty \\frac{1 - \\gamma^T}{1-\\gamma} \\cdot d_{TV}(b, b')$$\n",
    "\n",
    "For $T \\to \\infty$, this converges to Theorem 4.1.\n",
    "\n",
    "### Corollary 5.3: Policy Robustness\n",
    "\n",
    "If a policy $\\pi$ is optimal for belief $b$, then for any $b'$ with $d_{TV}(b, b') \\leq \\epsilon$, the value under policy $\\pi$ at $b'$ is:\n",
    "$$V^\\pi(b') \\geq V^*(b') - 2 \\frac{\\|r\\|_\\infty}{1-\\gamma} \\epsilon$$\n",
    "\n",
    "(The factor of 2 comes from: $V^*(b') \\geq V^\\pi(b') \\geq V^\\pi(b) - \\text{bound}$ and $V^\\pi(b) \\geq V^*(b) - \\text{bound}$)\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Assumptions Discussion\n",
    "\n",
    "### Why Assumption A3 (Non-degeneracy)?\n",
    "\n",
    "**Without Assumption A3**, the observation probability $p(o|b,a)$ could become arbitrarily small. This creates problems:\n",
    "\n",
    "1. **Belief updates become unstable:** $\\tau_a(b,o) = \\mu / p(o|b,a)$ requires dividing by a small number\n",
    "2. **Normalization fails:** The constants in Lemma 3.2 blow up\n",
    "3. **Theorem fails:** The Lipschitz constant becomes infinite\n",
    "\n",
    "**Assumption A3 ensures**: The denominator is always bounded below, so normalization is stable.\n",
    "\n",
    "### Can we weaken the assumptions?\n",
    "\n",
    "Yes, but at a cost:\n",
    "\n",
    "- **Weaker non-degeneracy:** Allow $p(o|b,a) = 0$ for some $(b,a,o)$ but require the set of zero-probability observations has measure zero. Then the theorem holds almost everywhere.\n",
    "\n",
    "- **Unbounded rewards:** If $\\|r\\|_\\infty = \\infty$, the Lipschitz constant becomes infinite. But we can still get finite bounds for \"typical\" states using local Lipschitz constants.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Optimality of the Constant\n",
    "\n",
    "**Theorem 4.2:** The constant $\\frac{\\|r\\|_\\infty}{1-\\gamma}$ is **tight** in the sense that there exist examples where:\n",
    "$$\\sup_{b \\neq b'} \\frac{|V^*(b) - V^*(b')|}{d_{TV}(b, b')} = \\frac{\\|r\\|_\\infty}{1-\\gamma}$$\n",
    "\n",
    "**Intuition:** This supremum is achieved when:\n",
    "- The beliefs differ maximally at the \"best\" state (where rewards are highest)\n",
    "- All future trajectories remain separated\n",
    "\n",
    "In such a scenario, the entire $\\frac{\\|r\\|_\\infty}{1-\\gamma}$ bound is utilized.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Comparison with Other Metrics\n",
    "\n",
    "The total variation distance is not the only metric on belief spaces. How do they compare?\n",
    "\n",
    "| Metric | Constant | Advantages | Disadvantages |\n",
    "|--------|----------|-----------|-----------------|\n",
    "| Total Variation (this work) | $\\frac{\\|r\\|_\\infty}{1-\\gamma}$ | Finite for all bounded $r$ | Can be loose for continuous spaces |\n",
    "| Wasserstein-1 | Depends on state space | Finer control | Requires metric on state space |\n",
    "| KL-divergence | Infinite for disjoint support | Separates more finely | Not a metric; asymmetric |\n",
    "| Hellinger | $2\\|r\\|_\\infty/(1-\\gamma)$ | Related to TV by Pinsker | Similar to TV |\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Conclusion\n",
    "\n",
    "We have proven that POMDP value functions are Lipschitz continuous in total variation distance with a concrete, computable constant. This result:\n",
    "\n",
    "1. **Quantifies robustness** to belief uncertainty\n",
    "2. **Provides bounds** on policy performance under model mismatch  \n",
    "3. **Enables approximation guarantees** for belief space discretization\n",
    "4. **Applies broadly** to any POMDP satisfying mild assumptions\n",
    "\n",
    "The proof structure (Lemma 1 → Lemma 2 → Lemma 3 → Main Theorem) is modular and can be extended to related settings (robust MDPs, partially observed games, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[Include citations to:]\n",
    "- Original POMDP papers (Kaelbling, Astrom, etc.)\n",
    "- Recent robustness papers (Kara & Yüksel, Demirci et al.)\n",
    "- General MDP Lipschitz results (Hinderer)\n",
    "- TV distance theory (Pollard, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposition 2 — Disentangled Representations Reduce Sample Complexity\n",
    "\n",
    "Assume a disentangled representation where:\n",
    "$$Z_t^i = g_i(F_t^i)$$\n",
    "each latent depends on one true factor.\n",
    "\n",
    "And assume the optimal policy depends only on factors in subset $J$:\n",
    "$$Q^*(Z_t, a) = \\tilde{Q}^*(Z_t^J, a)$$\n",
    "\n",
    "Then by standard statistical learning theory, the sample complexity to learn $\\tilde{Q}$ scales as:\n",
    "$$\\text{Sample complexity} \\propto \\sqrt{\\frac{d_{\\text{eff}}}{N}}$$\n",
    "\n",
    "where $d_{\\text{eff}} = |J| \\leq d$.\n",
    "\n",
    "Sample complexity as a justification for disentanglement — From a learning theory perspective:\n",
    "\n",
    "Learning with many correlated features = learning with inflated $d_{\\text{eff}}$\n",
    "\n",
    "Learning with disentangled factors = learning with reduced $d_{\\text{eff}}$\n",
    "\n",
    "The latter requires fewer samples to achieve the same generalization error\n",
    "\n",
    "### Proof Sketch\n",
    "\n",
    "Disentanglement ensures that only $|J|$ dimensions are relevant to the control problem. Standard VC/Rademacher complexity results show sample complexity scales with the number of relevant dimensions, not total dimensions. For entangled representations, the value function must encode all $d$ dimensions to discriminate the true factors, giving $d_{\\text{eff}} \\approx d$.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "MoE-based disentanglement directly reduces the effective dimensionality of value function learning, improving both sample efficiency and approximation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposition 3 — Dissimilar Tasks Benefit More from MoE\n",
    "\n",
    "### Setup\n",
    "Approximate each task's loss quadratically around its optimum:\n",
    "$$\\mathcal{L}_i(\\theta) \\approx \\frac{1}{2}(\\theta - \\theta_i^*)^\\top H_i (\\theta - \\theta_i^*)$$\n",
    "\n",
    "where $\\theta_i^*$ is task $i$'s optimal parameters and $H_i$ is the Hessian (curvature).\n",
    "\n",
    "### Single Shared Model\n",
    "$$\\theta_{\\text{shared}} = \\arg\\min_\\theta \\sum_{i=1}^N \\mathcal{L}_i(\\theta)$$\n",
    "\n",
    "Bias for task $j$:\n",
    "$$\\text{Bias}_j = \\mathcal{L}_j(\\theta_{\\text{shared}}) - \\mathcal{L}_j(\\theta_j^*) \n",
    "  \\approx \\frac{1}{2}(\\theta_{\\text{shared}} - \\theta_j^*)^\\top H_j (\\theta_{\\text{shared}} - \\theta_j^*)$$\n",
    "\n",
    "### MoE Model\n",
    "K experts with parameters $\\theta^{(1)}, \\ldots, \\theta^{(K)}$, with routing that (approximately) selects the best expert for task $j$:\n",
    "\n",
    "$$\\text{Bias}_j^{\\text{MoE}} \\approx \\frac{1}{2} \\min_k (\\theta^{(k)} - \\theta_j^*)^\\top H_j (\\theta^{(k)} - \\theta_j^*)$$\n",
    "\n",
    "### Bias Reduction Gain\n",
    "$$\\Delta_j = \\text{Bias}_j - \\text{Bias}_j^{\\text{MoE}} \n",
    "  \\approx \\frac{1}{2}\\left(\\|\\theta_{\\text{shared}} - \\theta_j^*\\|_{H_j}^2 - \\min_k \\|\\theta^{(k)} - \\theta_j^*\\|_{H_j}^2\\right)$$\n",
    "\n",
    "### Key Insights\n",
    "- **Dissimilar tasks gain more:** If task $j$ is far from the source distribution ($\\|\\theta_{\\text{shared}} - \\theta_j^*\\|$ large), then $\\Delta_j$ is large\n",
    "- **Task curvature matters:** Tasks with steep loss landscapes (large $\\|H_j\\|$) benefit more from MoE\n",
    "- **Requires expert specialization:** Gain only materializes if experts actually specialize to different task regions (requires diversity regularization or structured multi-task learning)\n",
    "\n",
    "### Caveat\n",
    "This analysis assumes:\n",
    "1. The quadratic approximation holds in a neighborhood of both $\\theta_{\\text{shared}}$ and $\\theta^{(k)}$\n",
    "2. Perfect routing (expert selection always picks $\\arg\\min_k$)\n",
    "3. The bias reduction gain outweighs the variance increase from additional parameters\n",
    "\n",
    "In practice, trade-off between bias reduction and model complexity via regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('hrssm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f7283e4099e674ce1a2c6afb904b708499aaa8b744778061c58955dd37a9ab7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
